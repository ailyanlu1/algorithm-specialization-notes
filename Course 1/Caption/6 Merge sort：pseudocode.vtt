WEBVTT

1
00:00:00.000 --> 00:00:03.095
Okay, so let's move on, and actually
discuss the pseudo-code for the

2
00:00:03.095 --> 00:00:06.744
merge sort algorithm. First, let me just
tell you the pseudo-code, leaving aside

3
00:00:06.744 --> 00:00:10.347
exactly how the merging subroutine is
implemented. And thus, high levels should

4
00:00:10.347 --> 00:00:14.182
be very simple and clear at this point. So
there's gonna be two recursive calls, and

5
00:00:14.182 --> 00:00:17.970
then there's gonna be a merging step. Now,
I owe you a few comments, 'cause I'm being

6
00:00:17.970 --> 00:00:21.480
a little sloppy. Again, as I promised,
this isn't something you would directly

7
00:00:21.480 --> 00:00:25.222
translate into code, although it's pretty
close. But so what are the couple of the

8
00:00:25.222 --> 00:00:28.779
ways that I'm being sloppy? Well, first of
all, there's, [inaudible], you know, in

9
00:00:28.779 --> 00:00:32.475
any recursive algorithm, you gotta have
some base cases. You gotta have this idea

10
00:00:32.475 --> 00:00:36.261
that when the input's sufficient. Really
small you don't do any recursion, you just

11
00:00:36.261 --> 00:00:39.906
return some trivial answer. So in the
sorting problem the base case would be if

12
00:00:39.906 --> 00:00:43.690
your handed an array that has either zero
or an elements, well it's already sorted,

13
00:00:43.690 --> 00:00:47.381
there's nothing to do, so you just return
it without any recursion. Okay, so to be

14
00:00:47.381 --> 00:00:51.120
clear, I haven't written down the base cases.
Although of course you would if you were

15
00:00:51.120 --> 00:00:54.826
actually implementing, a merge short. Some
of you, make a note of that. A couple of

16
00:00:54.826 --> 00:00:58.603
other things I'm ignoring. I'm ignoring
what the, what to do if the array has odd

17
00:00:58.603 --> 00:01:02.380
lengths, so if it has say nine elements,
obviously you have to somehow break that

18
00:01:02.380 --> 00:01:06.156
into five and four or four and five, so
you would do that just in either way and

19
00:01:06.156 --> 00:01:09.792
that would fine. And then secondly, I'm
ignoring the details or what it really

20
00:01:09.792 --> 00:01:13.569
means to sort of recursively sort, so for
example, I'm not discussing exactly how

21
00:01:13.569 --> 00:01:17.251
you would pass these subarrays onto the
recursive calls. That's something that

22
00:01:17.251 --> 00:01:20.792
would really depend somewhat on what, on
the programming language, so that's

23
00:01:20.792 --> 00:01:24.380
exactly what I want to avoid. I really
want to talk about the concepts which

24
00:01:24.380 --> 00:01:28.350
transcend any particular programming
language implementation. So that's why I'm

25
00:01:28.350 --> 00:01:33.262
going to describe algorithms at this level
okay. Alright, so the hard part relatively

26
00:01:33.262 --> 00:01:37.318
speaking, that is. How do you implement
the merge depth? The recursive calls have

27
00:01:37.318 --> 00:01:41.734
done their work. We have these two sort of
separated half the numbers. The left half

28
00:01:41.734 --> 00:01:45.893
and the right half. How do we combine them
into one? And in English, I already told

29
00:01:45.893 --> 00:01:50.052
you on the last slide. The idea is you
just populate the output array in a sorted

30
00:01:50.052 --> 00:01:53.852
order, by traversing pointers or just
traversing through the two, sorted

31
00:01:53.852 --> 00:01:58.062
sub-arrays in parallel. So let's look at
that in some more detail. Okay, so here is

32
00:01:58.062 --> 00:02:02.760
the pseudo-code for the merge step.
[sound] So let me begin by, introducing

33
00:02:02.760 --> 00:02:07.046
some names for the, characters in the,
what we're about to discuss. So let's use

34
00:02:07.046 --> 00:02:11.280
C. To denote the output array. So this is
what we're suppose to spit out with the

35
00:02:11.280 --> 00:02:15.626
numbers in sorted order. And then, I'm
gonna use a and b to denote the results of

36
00:02:15.626 --> 00:02:19.910
the two recursive calls, okay? So, the
first recursive call has given us array a,

37
00:02:19.910 --> 00:02:24.085
which contains the left half of the input
array in sorted order. Similarly, b

38
00:02:24.085 --> 00:02:28.094
contains the right half of the input
array, again, in sorted order. So, as I

39
00:02:28.094 --> 00:02:32.159
said, we're gonna need to traverse the
two, sorted sub-arrays, a and b, in

40
00:02:32.159 --> 00:02:36.059
parallel. So, I'm gonna introduce a
counter, i, to traverse through a, j to

41
00:02:36.059 --> 00:02:40.563
traverse through b. I and j will both be
initialized to one, to be at the beginning

42
00:02:40.563 --> 00:02:44.934
of their respective arrays. And now we're
gonna do. We're going to do a single pass

43
00:02:44.934 --> 00:02:48.791
of the output array copying it in an
increasing order. Always taking the

44
00:02:48.791 --> 00:02:52.800
smallest from the union of the two sorted
sub arrays. And if you, if there's one

45
00:02:52.800 --> 00:02:56.730
idea in this merge step it's just the
realization that. The minimum element that

46
00:02:56.730 --> 00:03:00.898
you haven't yet looked at in A and B has
to be at the front of one or the two lists

47
00:03:00.898 --> 00:03:04.917
right so for example at the very beginning
of the algorithm where is the minimum

48
00:03:04.917 --> 00:03:08.837
element over all. Well, which ever of the
two arrays it lands in -- A or B -- it has to be

49
00:03:08.837 --> 00:03:12.558
the smallest one there okay. So the
smallest element over all is either the

50
00:03:12.558 --> 00:03:16.528
smallest element A or it's the smallest
element B. So you just check both places,

51
00:03:16.528 --> 00:03:20.520
the smaller one is the smallest you copy
it over and you repeat. That's it. So the

52
00:03:20.520 --> 00:03:24.005
purpose of K is just to traverse the
output array from left to right. That's

53
00:03:24.005 --> 00:03:27.398
the order we're gonna populate it.
Currently looking at position I, and the

54
00:03:27.398 --> 00:03:31.157
first array of position J and the second
array. So that's how far we've gotten, how

55
00:03:31.157 --> 00:03:34.871
deeply we've probed in the both of those
two arrays. We look at which one has the

56
00:03:34.871 --> 00:03:38.981
current smallest, and we copy the smallest
one over. Okay? So if the, if, the entry

57
00:03:38.981 --> 00:03:43.438
in the i position of A is smaller, we
copy that one over. Of course, we have to

58
00:03:43.438 --> 00:03:48.009
increment i. We probe one deeper into the
list A, and symmeterically for the case

59
00:03:48.009 --> 00:03:52.466
where the current position in B has the
smaller element. Now again, I'm being a

60
00:03:52.466 --> 00:03:57.144
little bit sloppy, so that we can focus on
the forest, and not sort of, And not get

61
00:03:57.144 --> 00:04:01.003
bogged down with the trees. I'm ignoring
some end cases, so if you really wanted to

62
00:04:01.003 --> 00:04:04.956
implement this, you'd have to add a little
bit, to keep track of when you fall off,

63
00:04:05.098 --> 00:04:08.863
either, either A or B. Because you have
additional checks for when i or j reaches

64
00:04:08.863 --> 00:04:12.722
the end of the array, at which point you
copy over all the remaining elements into

65
00:04:12.722 --> 00:04:16.298
C. Alright, so I'm gonna give you a
cleaned up version, of, that pseudo-code

66
00:04:16.298 --> 00:04:20.016
so that you don't have to tolerate my
questionable handwriting any longer than

67
00:04:20.016 --> 00:04:23.734
is absolutely necessary. This again, is
just the same thing that we wrote on the

68
00:04:23.734 --> 00:04:27.499
last slide, okay? The pseudo-code for the
merge step. Now, so that's the Merge Sort

69
00:04:27.499 --> 00:04:32.886
algorithm. Now let's get to the meaty part
of this lecture, which is, okay, so merge

70
00:04:32.886 --> 00:04:38.633
sort produces a sorted array. What makes
it, if anything, better than much simpler

71
00:04:38.633 --> 00:04:44.034
non divide and conquer algorithms, like
say, insertion sort? Other words, what is

72
00:04:44.034 --> 00:04:48.413
the running time of the merge sort
algorithm? Now I'm not gonna give you a

73
00:04:48.413 --> 00:04:52.073
completely precise definition, definition
of what I mean by running time and there's

74
00:04:52.073 --> 00:04:55.514
good reason for that, as we'll discuss
shortly. But intuitively, you should think

75
00:04:55.514 --> 00:04:59.000
of the running time of an algorithm, you
should imagine that you're just running

76
00:04:59.000 --> 00:05:02.959
the algorithm in a debugger. Then, every
time you press enter, you advance with one

77
00:05:02.959 --> 00:05:07.095
line of the program through the debugger.
And then basically, the running time is

78
00:05:07.095 --> 00:05:11.283
just a number of operations executed, the
number of lines of code executed. So the

79
00:05:11.283 --> 00:05:15.212
question is, how many times you have to
hit enter on the debugger before the,

80
00:05:15.367 --> 00:05:19.554
program finally terminates. So we're
interested in how many such, lines of code

81
00:05:19.554 --> 00:05:23.172
get executed for Merge Short when
an input array has n numbers. Okay, so

82
00:05:23.172 --> 00:05:27.197
that's a fairly complicated question. So
let's start with a more modest school.

83
00:05:27.197 --> 00:05:31.377
Rather than thinking about the number of
operations executed by Merge Sort, which

84
00:05:31.377 --> 00:05:35.557
is this crazy recursive algorithm, which
is calling itself over and over and over

85
00:05:35.557 --> 00:05:39.685
again. Let's just think about how many
operations are gonna get executed when we

86
00:05:39.685 --> 00:05:43.555
do a single merge of two sorted sub
arrays. That seems like it should be an

87
00:05:43.555 --> 00:05:47.528
easier place to start. So let me remind
you, the pseudo code of the merge

88
00:05:47.528 --> 00:05:51.244
subroutine, here it is. So let's just go
and count up how many operations

89
00:05:51.244 --> 00:05:56.117
that are gonna get used. So there's
the initialization step. So let's say that

90
00:05:56.117 --> 00:06:02.009
I'm gonna charge us one operation for each
of these two initializations. So let's

91
00:06:02.009 --> 00:06:08.075
call this two operations, just set i equal to one
and j equal to one then we have this four

92
00:06:08.075 --> 00:06:13.925
loop executes a total number of end times
so each of these in iterations of this

93
00:06:13.925 --> 00:06:19.485
four loop how many instructions get
executed, well we have one here we have a

94
00:06:19.485 --> 00:06:25.334
comparison so we compare A(i) to B(j) and
either way the comparison comes up we then

95
00:06:25.334 --> 00:06:31.809
do two more operations, we do an
assignment. Here or here. And then we do

96
00:06:31.809 --> 00:06:37.197
an increment of the relevent variable
either here or here. So that's gonna be

97
00:06:37.197 --> 00:06:42.795
three operations per iteration. And then
maybe I'll also say that in order to

98
00:06:42.795 --> 00:06:48.533
increment K we're gonna call it a fourth
iteration. Okay? So for each of these N

99
00:06:48.533 --> 00:06:54.341
iterations of the four loop we're gonna do
four operations. All right? So putting it

100
00:06:54.341 --> 00:06:59.660
all together, what do we have is the
running time for merge. So let's see the

101
00:06:59.660 --> 00:07:04.453
upshot. So the upshot is that the running
time of the merge subroutine, given an

102
00:07:04.453 --> 00:07:09.174
array of M numbers, is at most four M plus
two. So a couple of comments. First of

103
00:07:09.174 --> 00:07:14.135
all, I've changed a letter on you so don't
get confused. In the previous slide we

104
00:07:14.135 --> 00:07:18.832
were thinking about an input size of N.
Here I've just made it. See I've changed

105
00:07:18.832 --> 00:07:22.514
the name of the variable to M. That's
gonna be convenient once we think about

106
00:07:22.514 --> 00:07:26.338
merge sort, which is recursing on smaller
sub-problems. But it's exactly the same

107
00:07:26.338 --> 00:07:30.020
thing and, and whatever. So an array of M
entries does as most four M plus two.

108
00:07:30.020 --> 00:07:34.325
Lines of code. The second thing is,
there's some ambiguity in exactly how we

109
00:07:34.325 --> 00:07:38.975
counted lines of code on the previous
slide. So maybe you might argue that, you

110
00:07:38.975 --> 00:07:42.936
know, really, each loop iteration should
count as two operations, not just

111
00:07:42.936 --> 00:07:47.242
one.'Cause you don't just have to
increment K, but you also have to compare

112
00:07:47.242 --> 00:07:51.547
it to the, upper bound of N. Eh, maybe.
Would have been 5M+2 instead of 4M+2. So

113
00:07:51.547 --> 00:07:56.517
it turns out these small differences in
how you count up. The number of lines of

114
00:07:56.517 --> 00:08:01.217
code executed are not gonna matter, and
we'll see why shortly. So, amongst

115
00:08:01.217 --> 00:08:06.375
friends, let's just agree, let's call it
4M plus two operations from merge, to

116
00:08:06.375 --> 00:08:11.728
execute on array on exactly M entries. So,
let me abuse our friendship now a little

117
00:08:11.728 --> 00:08:17.016
bit further with an, an inequality which is
true, but extremely sloppy. But I promise

118
00:08:17.016 --> 00:08:22.173
it'll make our lives just easier in some
future calculations. So rather than 4m+2,

119
00:08:22.173 --> 00:08:28.250
'cause 2's sorta getting on my nerves.
Let's just call this. Utmost six N.

120
00:08:28.250 --> 00:08:35.256
Because m is at least one. [sound] Okay,
you have to admit it's true, 6MO is at

121
00:08:35.256 --> 00:08:40.108
least 4M plus two. It's very sloppy, these
numbers are not anything closer to each

122
00:08:40.108 --> 00:08:45.021
other for M large but, let's just go ahead
and be sloppy in the interest of future

123
00:08:45.021 --> 00:08:49.456
simplicity. Okay. Now I don't expect
anyone to be impressed with this rather

124
00:08:49.456 --> 00:08:53.789
crude upper bound, the number of lines of
code that the merge subroutine needs to

125
00:08:53.789 --> 00:08:57.961
finish, to execute. The key question you
recall was how many lines of code does

126
00:08:57.961 --> 00:09:02.169
merge sort require to correctly sort the
input array, not just this subroutine. And

127
00:09:02.169 --> 00:09:06.196
in fact, analyzing Merge Sort seems a lot
more intimidating, because if it keeps

128
00:09:06.196 --> 00:09:10.325
spawning off these recursive versions of
itself. So the number of recursive calls,

129
00:09:10.325 --> 00:09:14.403
the number of things we have to analyze,
is blowing up exponentially as we think

130
00:09:14.403 --> 00:09:18.328
about various levels of the recursion.
Now, if there's one thing we have going

131
00:09:18.328 --> 00:09:22.137
for us, it's that every time we make a
recursive call. It's on a quite a bit

132
00:09:22.137 --> 00:09:26.317
smaller input then what we started with,
it's on an array only half the size of the

133
00:09:26.317 --> 00:09:30.396
input array. So there's some kind of
tension between on the one hand explosion

134
00:09:30.396 --> 00:09:34.313
of sub problems, a proliferation of sub
problems and the fact that successive

135
00:09:34.313 --> 00:09:38.412
subproblems only have to solve smaller and
smaller subproblems. And resolute

136
00:09:38.412 --> 00:09:42.888
resolving these two forces is what's going
to drive our analysis of Merge Short. So,

137
00:09:42.888 --> 00:09:47.203
the good news is, is I'll be able to show
you a complete analysis of exactly how

138
00:09:47.203 --> 00:09:51.518
many lines of code Merge Sort takes. And
I'll be able to give you, and, in fact, a

139
00:09:51.518 --> 00:09:55.886
very precise upper bound. And so here's
gonna be the claim that we're gonna prove

140
00:09:55.886 --> 00:10:00.363
in the remainder of this lecture. So the
claim is that Merge Short never needs than

141
00:10:00.363 --> 00:10:05.052
more than six times N. Times the logarithm
of N log base two if you're keeping track

142
00:10:05.052 --> 00:10:08.949
plus an extra six N operations to
correctly sort an input array of N

143
00:10:08.949 --> 00:10:13.581
numbers, okay so lets discuss for a second
is this good is this a win, knowing that

144
00:10:13.751 --> 00:10:18.439
this is an upper bound of the number of
lines of code the merger takes well yes it

145
00:10:18.439 --> 00:10:23.026
is and it shows the benefits of the divide
and conquer paradigm. Recall. In the

146
00:10:23.026 --> 00:10:27.636
simpler sorting methods that we briefly
discussed like insertion sort, selection

147
00:10:27.636 --> 00:10:31.901
sort, and bubble sort, I claimed that
their performance was governed by the

148
00:10:31.901 --> 00:10:36.454
quadratic function of the input size. That
is they need a constant times in the

149
00:10:36.454 --> 00:10:40.834
squared number of operations to sort an
input array of length N. Merge sort by

150
00:10:40.834 --> 00:10:45.445
contrast needs at most a constant times N
times log N, not N squared but N times

151
00:10:45.445 --> 00:10:50.030
log N lines of code to correctly sort an
input array. So to get a feel for what

152
00:10:50.030 --> 00:10:54.796
kind of win this is let me just remind you
for those of you who are rusty, or for

153
00:10:54.796 --> 00:10:59.622
whatever reason have lived in fear of a
logarithm, just exactly what the logarithm

154
00:10:59.622 --> 00:11:05.491
is. Okay? So. The way to think about the
logarithm is as follows. So you have the X

155
00:11:05.491 --> 00:11:12.099
axis, where you have N, which is going
from one up to infinity. And for

156
00:11:12.099 --> 00:11:19.165
comparison let's think about just the
identity function, okay? So, the function

157
00:11:19.165 --> 00:11:27.244
which is just. F(n)=n. Okay, and
let's contrast this with a logarithm. So

158
00:11:27.244 --> 00:11:31.267
what is the logorithm? Well, for our
purposes, we can just think of a logorithm

159
00:11:31.267 --> 00:11:36.041
as follows, okay? So the log of n, log
base 2 of n is, you type the number N

160
00:11:36.041 --> 00:11:40.956
into your calculator, okay? Then you hit
divide by two. And then you keep repeating

161
00:11:40.956 --> 00:11:44.888
dividing by two and you count how many
times you divide by two until you get a

162
00:11:44.888 --> 00:11:50.229
number that drops below one okay. So if
you plug in 32 you got to divide five

163
00:11:50.229 --> 00:11:56.359
times by two to get down to one. Log base two of
32 is five. You put in 1024 you have to

164
00:11:56.359 --> 00:12:01.189
divide by two, ten times till you get down
to one. So log base two of 1024 is ten and

165
00:12:01.189 --> 00:12:06.343
so on, okay. So the point is you already
see this if a log of a 1000 roughly is

166
00:12:06.343 --> 00:12:12.056
something like ten then the logarithm is
much, much smaller than the input.

167
00:12:12.056 --> 00:12:18.186
So graphically, what the logarithm is going
to look like is it's going to look like. A

168
00:12:18.186 --> 00:12:27.317
curve becomes very flat very quickly, as N
grows large, okay? So F(n) being log

169
00:12:27.317 --> 00:12:30.552
base 2 of n. And I encourage you to
do this, perhaps a little bit more

170
00:12:30.552 --> 00:12:34.031
precisely on the computer or a graphing
calculator, at home. But log is

171
00:12:34.031 --> 00:12:38.359
running much, much, much slower than
the identity function. And as a result,

172
00:12:38.359 --> 00:12:42.600
sorting algorithm which runs in time
proportional to n times log n is much,

173
00:12:42.600 --> 00:12:47.419
much faster, especially as n grows large, than a sorting algorithm with a

174
00:12:47.419 --> 00:12:51.538
running time that's a constant times n
squared.