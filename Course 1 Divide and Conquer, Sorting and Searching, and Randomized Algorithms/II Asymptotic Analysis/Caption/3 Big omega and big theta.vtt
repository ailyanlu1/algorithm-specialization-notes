WEBVTT

1
00:00:00.872 --> 00:00:03.935
In this lecture, we'll continue our formal
treatment of asymptotic notation. We've

2
00:00:03.935 --> 00:00:07.307
already discussed big O notation, which is
by far the most important and ubiquitous

3
00:00:07.307 --> 00:00:11.489
concept that's part of asymptotic notation,
but, for completeness, I do want to tell you

4
00:00:11.489 --> 00:00:16.655
about a couple of close relatives of big
O, namely omega and theta. If big O is

5
00:00:16.655 --> 00:00:21.422
analogous to less than or equal to, then
omega and theta are analogous to greater

6
00:00:21.422 --> 00:00:24.785
than or equal to, and equal to,
respectively. But let's treat them a

7
00:00:24.785 --> 00:00:28.373
little more precisely. The formal
definition of omega notation closely

8
00:00:28.373 --> 00:00:33.370
mirrors that of big O notation. We say
that one function, T of N, is big omega of

9
00:00:33.370 --> 00:00:36.690
another function, F of N, if
eventually, that is for sufficiently large

10
00:00:36.690 --> 00:00:42.642
N, it's lower bounded by a constant multiple
of F of N. And we quantify the ideas of a

11
00:00:42.642 --> 00:00:46.851
constant multiple and eventually in
exactly the same way as before, namely via

12
00:00:46.851 --> 00:00:52.316
explicitly giving two constants, C and N naught, such that T of N is bounded below by

13
00:00:52.316 --> 00:00:58.471
C times F of N for all sufficiently large
N. That is, for all N at least N naught.

14
00:00:58.471 --> 00:01:02.535
There's a picture just like there was for
big O notation. Perhaps we have a function

15
00:01:02.535 --> 00:01:06.289
T of N which looks something like this
green curve. And then we have another

16
00:01:06.289 --> 00:01:12.592
function F of N which is above T of N. But
then when we multiply F of N by one half,

17
00:01:12.592 --> 00:01:18.192
we get something that, eventually, is
always below T of N. So in this picture,

18
00:01:18.192 --> 00:01:24.416
this is an example where T of N is indeed
big Omega of F of N. As far as what the

19
00:01:24.416 --> 00:01:30.024
constants are, well, the multiple that we
use, C, is obviously just one half. That's

20
00:01:30.024 --> 00:01:33.566
what we're multiplying F of N by. And as before, N naught is the

21
00:01:33.566 --> 00:01:39.417
crossing point between the two functions.
So, N naught is the point after which C

22
00:01:39.417 --> 00:01:44.892
times F of N always lies below T of N
forevermore. So that's Big Omega. Theta

23
00:01:44.892 --> 00:01:47.870
notation is the equivalent of
equals, and so it just means that the

24
00:01:47.870 --> 00:01:52.892
function is both Big O of F of N and Omega
of F of N. An equivalent way to think

25
00:01:52.892 --> 00:01:57.424
about this is that, eventually, T of N is
sandwiched between two different constant

26
00:01:57.424 --> 00:02:01.493
multiples of F of N. I'll write that down,
and I'll leave it to you to verify that

27
00:02:01.493 --> 00:02:06.575
the two notions are equivalent. That is,
one implies the other and vice versa. So

28
00:02:06.575 --> 00:02:10.738
what do I mean by T of N is eventually
sandwiched between two multiples of F of N?

29
00:02:10.738 --> 00:02:15.356
Well, I just mean we choose two constants. A small one, C1, and a big constant, C2,

30
00:02:15.356 --> 00:02:20.507
and for all N at least N naught, T of N lies between
those two constant multiples.

31
00:02:20.507 --> 00:02:28.251
One way that algorithm designers can be quite sloppy is by using O notation instead of theta notation.

32
00:02:28.251 --> 00:02:32.285
So that's a common convention and I will follow that convention often in this class.

33
00:02:32.285 --> 00:02:36.218
Let me give you an example. Suppose we have a subroutine, which does a linear scan

34
00:02:36.218 --> 00:02:41.426
through an array of length N. It looks at each entry in the array and does a constant amount of work

35
00:02:41.426 --> 00:02:46.702
with each entry. So the merge subroutine would be more or less an example of a subroutine of that type.

36
00:02:46.702 --> 00:02:50.537
So even though the running time of such an algorithm, a subroutine, is patently

37
00:02:50.537 --> 00:02:55.351
theta of N, it does constant work for each of N entries, so it's exactly theta of N,

38
00:02:55.351 --> 00:02:59.168
we'll often just say that it has running time O of N. We won't bother

39
00:02:59.168 --> 00:03:02.738
to make the stronger statement that it's theta of N. The reason we do that is because

40
00:03:02.738 --> 00:03:05.471
you know, as algorithm designers, what we really care about is upper bounds.

41
00:03:05.471 --> 00:03:08.042
We want guarantees on how long our algorithms are going to run,

42
00:03:08.042 --> 00:03:11.853
so naturally we focus on the upper bounds and not so much on the lower bound side.

43
00:03:11.853 --> 00:03:16.552
So don't get confused. Once in a while, there will a quantity which is obviously theta of F of N,

44
00:03:16.552 --> 00:03:20.190
and I'll just make the weaker statement that it's O of F of N.

45
00:03:20.190 --> 00:03:23.892
The next quiz is meant to check your understanding
of these three concepts:

46
00:03:23.892 --> 00:03:26.585
Big O, Big Omega, and Big Theta notation.

47
00:03:29.000 --> 00:03:31.128
So the final three responses are all correct, and I

48
00:03:31.128 --> 00:03:35.671
hope the high level intuition for why is
fairly clear. T of N is definitely a

49
00:03:35.671 --> 00:03:39.541
quadratic function. We know that the
linear term doesn't matter much as it

50
00:03:39.541 --> 00:03:43.592
grows, as N grows large. So since it has
quadratic growth, then the third response

51
00:03:43.592 --> 00:03:48.635
should be correct. It's theta of N
squared. And it is omega of N. So Omega

52
00:03:48.635 --> 00:03:52.422
of N is not a very good lower bound on
the asymptotic rate of growth of T of N,

53
00:03:52.422 --> 00:03:56.107
but it is legitimate. Indeed, as a
quadratic growing function, it grows

54
00:03:56.107 --> 00:04:00.471
at least as fast as a linear function. So
it's Omega of N. For the same reason, big

55
00:04:00.471 --> 00:04:04.069
O of N cubed, it's not a very good upper
bound, but it is a legitimate one, it is

56
00:04:04.069 --> 00:04:07.818
correct. The rate of growth of T of N is
at most cubic. In fact, it's at most

57
00:04:07.818 --> 00:04:12.059
quadratic, but it is indeed, at most,
cubic. Now if you wanted to prove these

58
00:04:12.059 --> 00:04:17.241
three statements formally, you would just
exhibit the appropriate constants. So for

59
00:04:17.241 --> 00:04:21.887
proving that it's big Omega of N,
you could take N naught equal to one, and

60
00:04:21.887 --> 00:04:25.638
C equal to one-half. For the final
statement, again you could take N naught

61
00:04:25.638 --> 00:04:30.788
equal to one. And C equal to say four. And
to prove that it's theta of N squared you

62
00:04:30.788 --> 00:04:35.037
could do something similar just using the
two constants combined. So N naught would

63
00:04:35.037 --> 00:04:41.154
be one. You could take C1 to be one-half and C2
to be four. And I'll leave it to you to

64
00:04:41.154 --> 00:04:45.773
verify that the formal definitions of big
omega, big theta, and big O would be

65
00:04:45.773 --> 00:04:50.407
satisfied with these choices of constants.
One final piece of asymptotic notation,

66
00:04:50.407 --> 00:04:53.597
we're are not going to use this much, but
you do see it from time to time so I

67
00:04:53.597 --> 00:04:57.000
wanted to mention it briefly. This is
called little O notation, in contrast to

68
00:04:57.000 --> 00:05:01.440
big O notation. So while big O notation
informally is a less than or equal to type

69
00:05:01.440 --> 00:05:06.292
relation, little O is a strictly less than
relation. So intuitively it means that one

70
00:05:06.292 --> 00:05:10.976
function is growing strictly less quickly
than another. So formally we say that a

71
00:05:10.976 --> 00:05:19.136
function T of N is  little O of F of N,
if and only if for all constants C, there is

72
00:05:19.136 --> 00:05:23.017
a constant N naught beyond which T of N is
upper bounded by this constant multiple C

73
00:05:23.017 --> 00:05:26.991
times by F of N. So the difference between
this definition and that of Big-O notation, is

74
00:05:26.991 --> 00:05:30.775
that, to prove that one function is big O of another, we only have to

75
00:05:30.775 --> 00:05:36.810
exhibit one measly constant C, such that C
times F of N is upper bound,

76
00:05:36.810 --> 00:05:40.260
eventually, for T of N. By contrast, to
prove that something is little O of

77
00:05:40.260 --> 00:05:44.320
another function, we have to prove
something quite a bit stronger. We have to

78
00:05:44.320 --> 00:05:48.156
prove that, for every single constant C,
no matter how small, for every C, there

79
00:05:48.156 --> 00:05:53.444
exists some large enough N naught beyond
which T of N is bounded above by C times F

80
00:05:53.444 --> 00:05:57.523
of N. So, for those of you looking for a
little more facility with little O

81
00:05:57.523 --> 00:06:03.043
notation, I'll leave it as an exercise to
prove that, as you'd expect for all

82
00:06:03.043 --> 00:06:12.211
polynomial powers K, in fact, N to the K
minus one is little O of N to the K. There

83
00:06:12.211 --> 00:06:16.188
is an analogous notion of little omega
notation expressing that one function

84
00:06:16.188 --> 00:06:19.372
grows strictly quicker than another. But
that one you don't see very often, and I'm

85
00:06:19.372 --> 00:06:23.527
not gonna say anything more about it. So
let me conclude this video with a quote

86
00:06:23.527 --> 00:06:28.144
from an article, back from 1976, about my
colleague Don Knuth, widely regarded as

87
00:06:28.144 --> 00:06:33.088
the grandfather of the formal analysis of
algorithms. And it's rare that you can

88
00:06:33.088 --> 00:06:37.528
pinpoint why and where some kind of
notation became universally adopted in the

89
00:06:37.528 --> 00:06:41.504
field. In the case of asymptotic notation, indeed, it's very clear where it came

90
00:06:41.504 --> 00:06:45.206
from. The notation was not invented by
algorithm designers or computer

91
00:06:45.206 --> 00:06:49.455
scientists. It's been in use in number
theory since the nineteenth century. But

92
00:06:49.455 --> 00:06:53.837
it was Don Knuth in '76 that proposed
that this become the standard language for

93
00:06:53.837 --> 00:06:57.126
discussing rate of growth,
and in particular, for the running time of

94
00:06:57.126 --> 00:07:00.293
algorithms. So in particular, he says in
this article, "On the basis of the issues

95
00:07:00.293 --> 00:07:04.492
discussed here, I propose that members of
SIGACT," this is the special

96
00:07:04.492 --> 00:07:07.819
interest group of the ACM, which is
concerned with theoretical computer

97
00:07:07.819 --> 00:07:11.524
science, in particular the analysis of
algorithms. So, "I propose that the members

98
00:07:11.524 --> 00:07:15.170
of SIGACT and editors in computer science
and mathematics journals adopt the O,

99
00:07:15.170 --> 00:07:19.155
omega, and theta notations as defined
above unless a better alternative can be

100
00:07:19.155 --> 00:07:24.028
found reasonably soon. So clearly a better
alternative was not found and ever since

101
00:07:24.028 --> 00:07:27.258
that time this has been the standard way
of discussing the rate of growth of

102
00:07:27.258 --> 00:07:31.200
running times of algorithms and that's
what we'll be using here.